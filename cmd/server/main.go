package main

import (
	"context"
	"fmt"
	"log/slog"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"

	"github.com/labstack/echo/v4"
	"github.com/labstack/echo/v4/middleware"
	echoSwagger "github.com/swaggo/echo-swagger"

	"github.com/aiservice/internal/cache"
	"github.com/aiservice/internal/config"
	"github.com/aiservice/internal/handlers"
	"github.com/aiservice/internal/log"
	"github.com/aiservice/internal/providers"
	"github.com/aiservice/internal/providers/gemini"
	"github.com/aiservice/internal/providers/mock"
	"github.com/aiservice/internal/services/analysis"
	"github.com/aiservice/internal/services/database"
	jobservice "github.com/aiservice/internal/services/jobService"

	_ "github.com/aiservice/docs" // docs is generated by Swag CLI, you have to import it.
)

// @title AIService API
// @version 1.0
// @description AI Service for processing board data with summarization and structurization capabilities
// @termsOfService http://swagger.io/terms/

// @contact.name API Support
// @contact.url http://www.swagger.io/support
// @contact.email support@swagger.io

// @license.name Apache 2.0
// @license.url http://www.apache.org/licenses/LICENSE-2.0.html

// @host localhost:8080
// @BasePath /
// @schemes http https
func main() {
	cfg := config.LoadFromEnv()

	_ = log.SetupJsonLogger()

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	llmClient := initLLMProviders(ctx, cfg)

	// Initialize cache
	appCache := cache.NewInMemoryCache()

	// Wrap LLM client with caching if enabled
	var wrappedLLMClient providers.LLMClient
	if cfg.Server.Env == "prod" {
		wrappedLLMClient = cache.NewCachedLLMClient(llmClient, appCache)
	} else {
		wrappedLLMClient = llmClient // Don't cache in dev to see fresh results
	}

	jobStorage, err := database.NewStorage(cfg.Database)
	if err != nil {
		slog.Error("failed to create storage:", "err", err)
		os.Exit(1) // Exit if storage creation fails
	}

	// Ensure database connection is closed on exit
	defer func() {
		slog.Info("closing db connection")
		if err := jobStorage.Close(); err != nil {
			slog.Error("failed to close storage:", "err", err)
		}
	}()

	// Wrap storage with caching if enabled
	var wrappedStorage jobservice.JobStorage
	if cfg.Server.Env == "prod" {
		wrappedStorage = cache.NewCachedJobStorage(jobStorage, appCache)
	} else {
		wrappedStorage = jobStorage // Don't cache in dev to see fresh results
	}

	// Create the analysis service without the job queue initially
	analysisService := analysis.NewAnalysisServiceWithoutJobQueue(cfg.Timeouts.SyncProcess, wrappedLLMClient)

	// Create the job queue service with the analysis service as the processor
	jobQueueService := jobservice.NewJobQueueService(
		cfg.Job.QueueSize,
		cfg.Job.WorkerCount,
		cfg.Job.DbWorkerCount,
		wrappedStorage,
		analysisService, // analysisService implements the Processor interface
	)

	// Now set the job queue service in the analysis service
	analysisService.SetJobQueueService(jobQueueService)

	e := echo.New()
	// Configure CORS based on environment
	var corsConfig middleware.CORSConfig
	if cfg.Server.Env == "prod" {
		// In production, only allow requests from the foggy backend
		corsConfig = middleware.CORSConfig{
			AllowOrigins:     []string{"http://localhost:3001", "http://backend:3001", "https://foggy-backend.example.com"}, // Adjust domain for actual production
			AllowMethods:     []string{http.MethodGet, http.MethodPost, http.MethodPut, http.MethodDelete, http.MethodOptions},
			AllowHeaders:     []string{echo.HeaderOrigin, echo.HeaderContentType, echo.HeaderAccept, echo.HeaderAuthorization},
			AllowCredentials: true,
		}
	} else {
		// In development, allow all origins
		corsConfig = middleware.CORSConfig{
			AllowOrigins: []string{"*"},
			AllowMethods: []string{http.MethodGet, http.MethodPost, http.MethodPut, http.MethodDelete, http.MethodOptions},
			AllowHeaders: []string{echo.HeaderContentType, echo.HeaderAuthorization, echo.HeaderOrigin, echo.HeaderAccept},
		}
	}

	e.Use(
		middleware.Logger(),
		middleware.Recover(),
		middleware.RequestID(),
		middleware.CORSWithConfig(corsConfig),
	)

	// Swagger documentation endpoint
	e.GET("/swagger/*", echoSwagger.WrapHandler)

	AnalyzeHandler := handlers.NewAnalyzeHandler(
		analysisService,
		jobQueueService,
		cfg.Timeouts.SyncProcess,
	)

	e.GET("/health", handlers.HealthHandler)
	e.GET("/jobs/:id", AnalyzeHandler.GetJobStatus)
	e.PUT("/jobs/:id/abort", AnalyzeHandler.Abort)
	e.POST("/summarize", AnalyzeHandler.Summarize)
	e.POST("/structurize", AnalyzeHandler.Structurize)

	startServer(ctx, cancel, cfg, jobQueueService, e)
}

func startServer(ctx context.Context, cancelAiServices context.CancelFunc, cfg *config.Config, jobQueueService *jobservice.JobQueueService, e *echo.Echo) {
	go func() {
		sigChan := make(chan os.Signal, 1)
		signal.Notify(sigChan, os.Interrupt, syscall.SIGTERM)
		<-sigChan

		slog.Info("shutting down...")

		cancelAiServices()

		ctx, cancelWithTimeout := context.WithTimeout(ctx, 10*time.Second)
		defer cancelWithTimeout()

		jobQueueService.Shutdown()

		if err := e.Shutdown(ctx); err != nil {
			slog.Error("shutdown error:", "err", err)
		}
	}()

	addr := fmt.Sprintf(":%s", cfg.Server.Port)
	slog.Info("starting server", "addr", addr, "env", cfg.Server.Env)

	if err := e.Start(addr); err != nil && err != http.ErrServerClosed {
		slog.Error("server error:", "err", err)
	}
}

func initLLMProviders(ctx context.Context, cfg *config.Config) providers.LLMClient {
	switch cfg.LLM.Provider {
	case "openai":
		// slog.Info("Using OpenAI LLM")
		// return openai.NewOpenAIClient(cfg.LLM)
		return nil
	case "gemini":
		if cfg.LLM.APIKey == "" || cfg.LLM.APIKey == "your_api_key_here" {
			slog.Warn("Gemini API key not provided or is default example value, using mock client")
			return mock.NewMockClient()
		}
		slog.Info("Using Gemini LLM")
		return gemini.NewGeminiClient(ctx, cfg.LLM)
	default:
		slog.Warn("Unknown LLM provider, using mock client", "provider", cfg.LLM.Provider)
		return mock.NewMockClient()
	}
}
